{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\igna-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import numpy as np\n",
    "import random as rd\n",
    "import nltk\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo y spliteo el set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('train_EN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tweets.iloc[:,1:3]\n",
    "train.dropna(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  sarcastic\n",
       "0     The only thing I got from college is a caffein...          1\n",
       "1     I love it when professors draw a big question ...          1\n",
       "2     Remember the hundred emails from companies whe...          1\n",
       "3     Today my pop-pop told me I was not “forced” to...          1\n",
       "4     @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
       "...                                                 ...        ...\n",
       "3463  The population spike in Chicago in 9 months is...          0\n",
       "3464  You'd think in the second to last English clas...          0\n",
       "3465  I’m finally surfacing after a holiday to Scotl...          0\n",
       "3466  Couldn't be prouder today. Well done to every ...          0\n",
       "3467  Overheard as my 13 year old games with a frien...          0\n",
       "\n",
       "[3467 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet'] = train['tweet'].str.lower()\n",
    "train['tweet'] = train['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo la matriz de embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " w2v_model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = np.concatenate([np.zeros((1,300)), \n",
    "                             np.random.normal(size=(1,300)),\n",
    "                             w2v_model.vectors[:500000]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['tweet'], train['sarcastic'], test_size=0.2, train_size=0.8, random_state=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.dropna()\n",
    "y_test = y_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 694/694 [00:00<00:00, 1493.71it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts_test = []\n",
    "for text in tqdm(X_test):\n",
    "    tokenized_texts_test.append(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2773/2773 [00:00<00:00, 3706.41it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "for text in tqdm(X_train):\n",
    "    tokenized_texts.append(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = int(np.percentile([len(t) for t in tokenized_texts], 95))\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_toks(tokenized_text):\n",
    "    tokenized_text =tokenized_text.copy()\n",
    "    if len(tokenized_text) > MAX_LEN:\n",
    "        tokenized_text = tokenized_text[:MAX_LEN]\n",
    "    for i in range(len(tokenized_text)):\n",
    "        if tokenized_text[i] in w2v_model and w2v_model.key_to_index[tokenized_text[i]] < 500000:\n",
    "            tokenized_text[i] = w2v_model.key_to_index[tokenized_text[i]]+2\n",
    "        else:\n",
    "            tokenized_text[i] = 1\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500002, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2773/2773 [00:00<00:00, 5063.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(tokenized_texts)), total=len(tokenized_texts)):\n",
    "    tokenized_texts[i] = preprocess_toks(tokenized_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenized_texts = pad_sequences(\n",
    "    tokenized_texts, maxlen=MAX_LEN, dtype='int32', padding='post',\n",
    "    truncating='post', value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GRU, Embedding, LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = Input((50,), dtype='int32')\n",
    "\n",
    "word_emb = Embedding(500002, 300, weights=[emb_matrix], \n",
    "                     trainable=False, mask_zero=True)\n",
    "\n",
    "emb_sequence = word_emb(word_indexes) # (22, 300)\n",
    "rnn1 = GRU(50, activation='tanh')(emb_sequence) # (1,50)\n",
    "dense1 = Dense(50, activation='tanh')(rnn1)\n",
    "dense2 = Dense(50, activation='tanh')(dense1)\n",
    "dense3 = Dense(50, activation='tanh')(dense2)\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(dense3)\n",
    "model = Model(inputs=word_indexes, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/93\n",
      "11/11 [==============================] - 18s 116ms/step - loss: 0.2003 - auc: 0.5054\n",
      "Epoch 2/93\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.1905 - auc: 0.5103\n",
      "Epoch 3/93\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.1894 - auc: 0.5327\n",
      "Epoch 4/93\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.1887 - auc: 0.5469\n",
      "Epoch 5/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1879 - auc: 0.5596\n",
      "Epoch 6/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.1863 - auc: 0.5912\n",
      "Epoch 7/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1841 - auc: 0.6183\n",
      "Epoch 8/93\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.1812 - auc: 0.6416\n",
      "Epoch 9/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1777 - auc: 0.6587\n",
      "Epoch 10/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.1748 - auc: 0.6735\n",
      "Epoch 11/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.1710 - auc: 0.6887\n",
      "Epoch 12/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.1655 - auc: 0.7140\n",
      "Epoch 13/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.1633 - auc: 0.7141\n",
      "Epoch 14/93\n",
      "11/11 [==============================] - 1s 104ms/step - loss: 0.1544 - auc: 0.7603\n",
      "Epoch 15/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.1492 - auc: 0.7752\n",
      "Epoch 16/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1448 - auc: 0.7839\n",
      "Epoch 17/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.1393 - auc: 0.8031\n",
      "Epoch 18/93\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.1353 - auc: 0.8087\n",
      "Epoch 19/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1282 - auc: 0.8290\n",
      "Epoch 20/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1218 - auc: 0.8413\n",
      "Epoch 21/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.1190 - auc: 0.8496\n",
      "Epoch 22/93\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.1213 - auc: 0.8441\n",
      "Epoch 23/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.1116 - auc: 0.8591\n",
      "Epoch 24/93\n",
      "11/11 [==============================] - 1s 102ms/step - loss: 0.0972 - auc: 0.8873\n",
      "Epoch 25/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0956 - auc: 0.8872\n",
      "Epoch 26/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0865 - auc: 0.8982\n",
      "Epoch 27/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.0786 - auc: 0.9051\n",
      "Epoch 28/93\n",
      "11/11 [==============================] - 1s 101ms/step - loss: 0.0704 - auc: 0.9137\n",
      "Epoch 29/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0643 - auc: 0.9204\n",
      "Epoch 30/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0603 - auc: 0.9209\n",
      "Epoch 31/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0570 - auc: 0.9234\n",
      "Epoch 32/93\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.0513 - auc: 0.9264\n",
      "Epoch 33/93\n",
      "11/11 [==============================] - 1s 103ms/step - loss: 0.0460 - auc: 0.9309\n",
      "Epoch 34/93\n",
      "11/11 [==============================] - 1s 105ms/step - loss: 0.0456 - auc: 0.9333\n",
      "Epoch 35/93\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.0424 - auc: 0.9349\n",
      "Epoch 36/93\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.0458 - auc: 0.9329\n",
      "Epoch 37/93\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.0392 - auc: 0.9386\n",
      "Epoch 38/93\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.0347 - auc: 0.9384\n",
      "Epoch 39/93\n",
      "11/11 [==============================] - 1s 127ms/step - loss: 0.0314 - auc: 0.9401\n",
      "Epoch 40/93\n",
      "11/11 [==============================] - 1s 124ms/step - loss: 0.0305 - auc: 0.9410\n",
      "Epoch 41/93\n",
      "11/11 [==============================] - 2s 139ms/step - loss: 0.0307 - auc: 0.9422\n",
      "Epoch 42/93\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 0.0289 - auc: 0.9434\n",
      "Epoch 43/93\n",
      "11/11 [==============================] - 1s 129ms/step - loss: 0.0285 - auc: 0.9418\n",
      "Epoch 44/93\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.0289 - auc: 0.9430\n",
      "Epoch 45/93\n",
      "11/11 [==============================] - 1s 129ms/step - loss: 0.0278 - auc: 0.9419\n",
      "Epoch 46/93\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.0273 - auc: 0.9431\n",
      "Epoch 47/93\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.0265 - auc: 0.9421\n",
      "Epoch 48/93\n",
      "11/11 [==============================] - 1s 132ms/step - loss: 0.0261 - auc: 0.9436\n",
      "Epoch 49/93\n",
      "11/11 [==============================] - 1s 129ms/step - loss: 0.0253 - auc: 0.9429\n",
      "Epoch 50/93\n",
      "11/11 [==============================] - 1s 123ms/step - loss: 0.0249 - auc: 0.9440\n",
      "Epoch 51/93\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0248 - auc: 0.9445\n",
      "Epoch 52/93\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 0.0247 - auc: 0.9435\n",
      "Epoch 53/93\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 0.0244 - auc: 0.9456\n",
      "Epoch 54/93\n",
      "11/11 [==============================] - 2s 177ms/step - loss: 0.0246 - auc: 0.9455\n",
      "Epoch 55/93\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0246 - auc: 0.9440\n",
      "Epoch 56/93\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 0.0243 - auc: 0.9461\n",
      "Epoch 57/93\n",
      "11/11 [==============================] - 2s 169ms/step - loss: 0.0244 - auc: 0.9452\n",
      "Epoch 58/93\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 0.0245 - auc: 0.9472\n",
      "Epoch 59/93\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0240 - auc: 0.9460\n",
      "Epoch 60/93\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 0.0237 - auc: 0.9474\n",
      "Epoch 61/93\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 0.0236 - auc: 0.9468\n",
      "Epoch 62/93\n",
      "11/11 [==============================] - 2s 172ms/step - loss: 0.0238 - auc: 0.9488\n",
      "Epoch 63/93\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 0.0237 - auc: 0.9478\n",
      "Epoch 64/93\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.0237 - auc: 0.9474\n",
      "Epoch 65/93\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 0.0235 - auc: 0.9470\n",
      "Epoch 66/93\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0233 - auc: 0.9491\n",
      "Epoch 67/93\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0234 - auc: 0.9482\n",
      "Epoch 68/93\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0229 - auc: 0.9497\n",
      "Epoch 69/93\n",
      "11/11 [==============================] - 2s 164ms/step - loss: 0.0230 - auc: 0.9491\n",
      "Epoch 70/93\n",
      "11/11 [==============================] - 2s 160ms/step - loss: 0.0231 - auc: 0.9465\n",
      "Epoch 71/93\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.0228 - auc: 0.9482\n",
      "Epoch 72/93\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.0228 - auc: 0.9467\n",
      "Epoch 73/93\n",
      "11/11 [==============================] - 2s 154ms/step - loss: 0.0228 - auc: 0.9487\n",
      "Epoch 74/93\n",
      "11/11 [==============================] - 2s 152ms/step - loss: 0.0228 - auc: 0.9469\n",
      "Epoch 75/93\n",
      "11/11 [==============================] - 2s 161ms/step - loss: 0.0228 - auc: 0.9474\n",
      "Epoch 76/93\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 0.0228 - auc: 0.9484\n",
      "Epoch 77/93\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 0.0227 - auc: 0.9480\n",
      "Epoch 78/93\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 0.0227 - auc: 0.9485\n",
      "Epoch 79/93\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 0.0227 - auc: 0.9482\n",
      "Epoch 80/93\n",
      "11/11 [==============================] - 2s 166ms/step - loss: 0.0227 - auc: 0.9480\n",
      "Epoch 81/93\n",
      "11/11 [==============================] - 2s 177ms/step - loss: 0.0229 - auc: 0.9480\n",
      "Epoch 82/93\n",
      "11/11 [==============================] - 2s 155ms/step - loss: 0.0228 - auc: 0.9503\n",
      "Epoch 83/93\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.0228 - auc: 0.9501\n",
      "Epoch 84/93\n",
      "11/11 [==============================] - 1s 116ms/step - loss: 0.0228 - auc: 0.9490\n",
      "Epoch 85/93\n",
      "11/11 [==============================] - 1s 107ms/step - loss: 0.0227 - auc: 0.9497\n",
      "Epoch 86/93\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.0227 - auc: 0.9496\n",
      "Epoch 87/93\n",
      "11/11 [==============================] - 1s 118ms/step - loss: 0.0227 - auc: 0.9498\n",
      "Epoch 88/93\n",
      "11/11 [==============================] - 1s 114ms/step - loss: 0.0227 - auc: 0.9498\n",
      "Epoch 89/93\n",
      "11/11 [==============================] - 1s 115ms/step - loss: 0.0227 - auc: 0.9508\n",
      "Epoch 90/93\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.0227 - auc: 0.9489\n",
      "Epoch 91/93\n",
      "11/11 [==============================] - 1s 111ms/step - loss: 0.0224 - auc: 0.9509\n",
      "Epoch 92/93\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.0224 - auc: 0.9516\n",
      "Epoch 93/93\n",
      "11/11 [==============================] - 1s 110ms/step - loss: 0.0226 - auc: 0.9518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b097a35610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tokenized_texts, y_train.values, epochs=93, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 694/694 [00:00<00:00, 1268.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(tokenized_texts_test)), total=len(tokenized_texts_test)):\n",
    "    tokenized_texts_test[i] = preprocess_toks(tokenized_texts_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_test = pad_sequences(\n",
    "    tokenized_texts_test, maxlen=MAX_LEN, dtype='int32', padding='post',\n",
    "    truncating='post', value=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenized_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (preds >= 0.5).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       532\n",
      "           1       0.33      0.26      0.29       162\n",
      "\n",
      "    accuracy                           0.70       694\n",
      "   macro avg       0.56      0.55      0.55       694\n",
      "weighted avg       0.68      0.70      0.69       694\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.597930010210712"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(classification_report(y_test, y_pred))\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo una nueva red con LSTM y un EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Concatenate, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = Input((50,), dtype='int32')\n",
    "\n",
    "word_emb = Embedding(500002, 300, weights=[emb_matrix], \n",
    "                     trainable=False, mask_zero=True)\n",
    "\n",
    "emb_sequence = word_emb(word_indexes)\n",
    "emb_sequence = Dropout(0.25)(emb_sequence)\n",
    "rnn1 = LSTM(50, activation='tanh')(emb_sequence)\n",
    "dense1 = Dense(50, activation='tanh')(rnn1)\n",
    "dense2 = Dense(50, activation='tanh')(dense1)\n",
    "dense3 = Dense(50, activation='tanh')(dense2)\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(dense3)\n",
    "model = Model(inputs=word_indexes, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras.metrics.AUC()])\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 42s 721ms/step - loss: 0.6069 - auc_15: 0.5160 - val_loss: 0.6419 - val_auc_15: 0.4392\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 4s 403ms/step - loss: 0.5820 - auc_15: 0.5000 - val_loss: 0.6565 - val_auc_15: 0.4344\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 0.5782 - auc_15: 0.4967 - val_loss: 0.6451 - val_auc_15: 0.4324\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 4s 391ms/step - loss: 0.5735 - auc_15: 0.4953 - val_loss: 0.6313 - val_auc_15: 0.4482\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 4s 395ms/step - loss: 0.5700 - auc_15: 0.5005 - val_loss: 0.6270 - val_auc_15: 0.4791\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 4s 415ms/step - loss: 0.5684 - auc_15: 0.4987 - val_loss: 0.6224 - val_auc_15: 0.5078\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 4s 428ms/step - loss: 0.5659 - auc_15: 0.5134 - val_loss: 0.6247 - val_auc_15: 0.5113\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.5637 - auc_15: 0.5296 - val_loss: 0.6208 - val_auc_15: 0.5226\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.5620 - auc_15: 0.5398 - val_loss: 0.6200 - val_auc_15: 0.5229\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 4s 405ms/step - loss: 0.5621 - auc_15: 0.5379 - val_loss: 0.6192 - val_auc_15: 0.5266\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 4s 406ms/step - loss: 0.5613 - auc_15: 0.5362 - val_loss: 0.6223 - val_auc_15: 0.5316\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 4s 414ms/step - loss: 0.5624 - auc_15: 0.5281 - val_loss: 0.6169 - val_auc_15: 0.5301\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 4s 392ms/step - loss: 0.5619 - auc_15: 0.5319 - val_loss: 0.6187 - val_auc_15: 0.5286\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 4s 401ms/step - loss: 0.5626 - auc_15: 0.5220 - val_loss: 0.6194 - val_auc_15: 0.5277\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 4s 396ms/step - loss: 0.5606 - auc_15: 0.5381 - val_loss: 0.6191 - val_auc_15: 0.5325\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 4s 399ms/step - loss: 0.5622 - auc_15: 0.5220 - val_loss: 0.6165 - val_auc_15: 0.5305\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 4s 406ms/step - loss: 0.5606 - auc_15: 0.5341 - val_loss: 0.6197 - val_auc_15: 0.5319\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 4s 429ms/step - loss: 0.5612 - auc_15: 0.5286 - val_loss: 0.6189 - val_auc_15: 0.5322\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 4s 416ms/step - loss: 0.5609 - auc_15: 0.5323 - val_loss: 0.6167 - val_auc_15: 0.5319\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 0.5596 - auc_15: 0.5438 - val_loss: 0.6196 - val_auc_15: 0.5281\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 0.5587 - auc_15: 0.5502 - val_loss: 0.6170 - val_auc_15: 0.5365\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 4s 424ms/step - loss: 0.5594 - auc_15: 0.5411 - val_loss: 0.6167 - val_auc_15: 0.5402\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tokenized_texts, y_train.values, validation_split=0.1, callbacks=[callback], epochs=100, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(history.history['loss']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenized_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (preds >= 0.5).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       532\n",
      "           1       0.00      0.00      0.00       162\n",
      "\n",
      "    accuracy                           0.77       694\n",
      "   macro avg       0.38      0.50      0.43       694\n",
      "weighted avg       0.59      0.77      0.67       694\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714981899192425"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la mejora del EarlyStopping y la LTSM empeoro.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebo Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ce7389095016>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mconv3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mconcatted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mcolap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcatted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdense1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdense2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\pooling.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_format, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGlobalPooling1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36mnormalize_data_format\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    207\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m   \u001b[0mdata_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'channels_last'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     raise ValueError('The `data_format` argument must be one of '\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "word_indexes = Input((50,), dtype='int32')\n",
    "\n",
    "word_emb = Embedding(500002, 300, weights=[emb_matrix], \n",
    "                     trainable=False, mask_zero=True)\n",
    "\n",
    "emb_sequence = word_emb(word_indexes)\n",
    "conv1 = Conv1D(10,1,1,padding = 'same')(emb_sequence)\n",
    "emb_sequence = Dropout(0.25)(emb_sequence)\n",
    "conv2 = Conv1D(10,2,1,padding = 'same')(emb_sequence)\n",
    "emb_sequence = Dropout(0.25)(emb_sequence)\n",
    "conv3 = Conv1D(10,3,1,padding = 'same')(emb_sequence)\n",
    "concatted = Concatenate()([conv1, conv2, conv3])\n",
    "colap = GlobalMaxPooling1D(2)(concatted)\n",
    "dense1 = Dense(30, activation='tanh')(colap)\n",
    "dense2 = Dense(30, activation='tanh')(dense1)\n",
    "dense3 = Dense(30, activation='tanh')(dense2)\n",
    "\n",
    "\n",
    "out = Dense(1, activation='sigmoid')(dense3)\n",
    "model = Model(inputs=word_indexes, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 50, 300)      150000600   ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 300)      0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 50, 10)       3010        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 50, 10)       6010        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 50, 10)       9010        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 50, 30)       0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d_1[0][0]',               \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 25, 30)       0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 25, 30)       930         ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 25, 30)       930         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 25, 30)       930         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 25, 1)        31          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 150,021,451\n",
      "Trainable params: 20,851\n",
      "Non-trainable params: 150,000,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2773, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[keras.metrics.AUC()])\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/93\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 25) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-6743f197033d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_texts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m93\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 817, in train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 460, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 73, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 177, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\metrics.py\", line 2343, in update_state  **\n        return metrics_utils.update_confusion_matrix_variables(\n    File \"C:\\Users\\igna-\\anaconda3\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 625, in update_confusion_matrix_variables\n        y_pred.shape.assert_is_compatible_with(y_true.shape)\n\n    ValueError: Shapes (None, 25) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model.fit(tokenized_texts,y_train.values.reshape(-1,1), epochs=93, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tokenized_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694, 25, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
